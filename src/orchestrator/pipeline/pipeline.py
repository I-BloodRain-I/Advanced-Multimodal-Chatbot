"""
Core pipeline orchestration module for the TensorAlix Agent AI system.

This module contains the main Pipeline class that coordinates the execution
of AI tasks through a multi-stage processing pipeline. It handles embedding
generation, task routing, retrieval-augmented generation, LLM inference,
and image generation in a unified workflow.

The pipeline processes batches of conversations, routing them to appropriate
task handlers based on classification results and returning structured responses.
"""

from typing import Dict, List, Literal
import logging

import torch

from core.entities.types import AgentResponse, TaskBatch
from core.entities.enums import ModelDType
from modules.image_generation import ImageGenerator
from modules.llm import LLM
from modules.llm.configs import LLMEngineConfig
from modules.rag import RAG, VectorDatabaseBase
from orchestrator.router import Router
from shared.embedders import Embedder
from shared.text_processing import PromptTransformer
from .utils import group_conversations_by_task, tensor_to_base64

from core.entities import ConversationBatch, TaskType

logger = logging.getLogger(__name__)


class Pipeline:
    """
    Singleton pipeline for processing batches of conversations through embedding, routing, and task execution.

    This class handles the high-level orchestration between core modules:
    - Embedder: Converts prompts into dense vector embeddings.
    - Router: Classifies each conversation into a task type.
    - RAG: Retrieves documents and formats prompts for LLM input.
    - LLM: Generates responses from a language model.
    - ImageGenerator: Creates image outputs from prompts when needed.

    Args:
        router_model: Pretrained classifier for task routing.
        llm_engine_name: The LLM engine type to use.
        llm_engine_config: Configuration object containing all LLM settings.
        rag_vector_db: Vector DB used in retrieval-augmented generation.
        rag_n_extracted_docs: Number of docs to retrieve per query (default 5).
        rag_prompt_format: Format string to merge retrieved docs and prompt.
        embed_model_name: Model name for the embedder.
        embed_device_name: Device name for running the embedder (e.g., 'cuda').
        img_model_name: Model name for the image generation model.
        img_device_name: Device for image generation (e.g., 'cuda').
        img_dtype: Data type for image model (e.g., 'fp16').
        img_scheduler_type: Type of scheduler to use during image generation.
        img_use_refiner: Whether to use an additional refiner pipeline.
        img_refiner_name: Model name of the image refiner pipeline.
    """
    _instance = None

    def __new__(cls, *args, **kwargs):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._initialized = False
        return cls._instance

    def __init__(self,
        router_model: torch.nn.Module,
        llm_engine_name: Literal['transformers', 'vllm'],
        llm_engine_config: LLMEngineConfig,
        rag_vector_db: VectorDatabaseBase,
        rag_n_extracted_docs: int = 5,
        rag_prompt_format: str = '{context}\n{prompt}',
        embed_model_name: str = 'all-mpnet-base-v2',
        embed_device_name: str = 'cuda',
        img_model_name: str = '',
        img_device_name: str = 'cuda',
        img_dtype: ModelDType = ModelDType.FLOAT16,
        img_scheduler_type: str = 'euler_ancestral',
        img_use_refiner: bool = False,
        img_refiner_name: str = 'stabilityai/stable-diffusion-xl-refiner-1.0'
    ):
        if self._initialized:
            return  # Avoid reinitialization
        
        # Instantiate core components
        self.embedder = Embedder(model_name=embed_model_name, device_name=embed_device_name)
        self.router = Router(model=router_model)
        self.rag = RAG(
            vector_db=rag_vector_db, 
            n_extracted_docs=rag_n_extracted_docs, 
            prompt_format=rag_prompt_format
        )
        self.llm = LLM(llm_engine_name, llm_engine_config)
        self.image_generator = ImageGenerator(
            model_name=img_model_name,
            device=img_device_name,
            dtype=img_dtype,
            scheduler_type=img_scheduler_type,
            use_refiner=img_use_refiner,
            refiner_name=img_refiner_name
        )
        self._initialized = True

    def __call__(self, batch: ConversationBatch) -> List[AgentResponse]:
        """
        Processes a batch of conversations end-to-end and returns agent responses.

        Args:
            batch: Batch of conversation histories and metadata.

        Returns:
            List of responses generated by the LLM.
        """
        try:
            grouped_convs = self._route_and_group(batch)
            return self._process_task_batches(grouped_convs)
        except Exception:
            logger.error(f"Pipeline execution failed.", exc_info=True)
            return []

    def _route_and_group(self, batch: ConversationBatch) -> Dict[TaskType, TaskBatch]:
        """
        Embeds prompts, classifies them by task type, and groups them accordingly.
        """
        #  Convert message history into string prompts
        prompts = PromptTransformer.format_messages_to_str_batch(batch.histories)
        # Generate dense embeddings for the prompts
        embeddings = self.embedder.extract_embeddings(prompts)
        # Classify the embeddings into task types
        task_types, probs = self.router.route(embeddings, return_probs=True)

        logger.debug(f"Routed task types: {[t.name for t in task_types]}, Probabilities: {probs.tolist()}")

        # Group inputs into batches by predicted task type
        return group_conversations_by_task(
            messages=batch.histories,
            task_types=task_types,
            conv_ids=batch.conv_ids,
            embeddings=embeddings
        )

    def _process_task_batches(self, grouped_convs: Dict[TaskType, TaskBatch]) -> List[AgentResponse]:
        """Processes each group of conversations based on its task type."""
        responses: List[AgentResponse] = []

        for task_type, group in grouped_convs.items():
            logger.debug(f"Handling {task_type.name} with {len(group.histories)} conversations")

            if task_type == TaskType.TEXT_GEN:
                responses.extend(self._handle_text_gen_task(group))
            elif task_type == TaskType.IMAGE_GEN:
                responses.extend(self._handle_img_gen_task(group))
            else:
                logger.warning(f"Unsupported task type: {task_type.name}")

        return responses

    def _handle_text_gen_task(self, group: TaskBatch) -> List[AgentResponse]:
        """Executes text generation logic: retrieval, prompt generation, and LLM response."""
        # Add retrieved documents to each conversation context
        self.rag.add_context(group.histories, group.embeddings)

        # Generate responses as a stream
        outputs = self.llm.generate_batch(group.histories)
        response_type = "stream"

        # Create AgentResponse objects for each output
        return [
            AgentResponse(conv_id=conv_id, type=response_type, content=content)
            for conv_id, content in zip(group.conv_ids, outputs)
        ]

    def _handle_img_gen_task(self, group: TaskBatch) -> List[AgentResponse]:
        """Executes image generation logic using prompts from user messages."""
        # Extract user prompts from the conversation history
        users_messages = PromptTransformer.get_messages_by_role(group.histories, role='user')
        prompts = PromptTransformer.get_content_from_messages(users_messages)

        # Generate images and convert them to base64
        imgs_tensor = self.image_generator.generate(prompts=prompts)
        outputs = [tensor_to_base64(img_tensor) for img_tensor in imgs_tensor]
        
        return [
            AgentResponse(conv_id=conv_id, type="image", content=content)
            for conv_id, content in zip(group.conv_ids, outputs)
        ]

    @classmethod
    def build(cls) -> 'Pipeline':
        """
        Builds and returns a pipeline instance using the configuration factory.

        Returns:
            Configured pipeline instance.
        """
        from orchestrator.pipeline.factory import PipelineFactory
        return PipelineFactory.build()