from typing import Dict, List
import logging

import torch

from core.entities.types import AgentResponse, TaskBatch
from modules.image_generation import ImageGenerator
from modules.llm import LLM
from modules.rag import RAG, VectorDatabaseBase
from orchestrator.router import Router
from shared.embedders import Embedder
from shared.text_processing import PromptTransformer
from .utils import group_conversations_by_task, tensor_to_base64

from core.entities import ConversationBatch, TaskType

logger = logging.getLogger(__name__)


class Pipeline:
    """
    Singleton pipeline for processing batches of conversations through embedding, routing, and task execution.

    This class handles the high-level orchestration between core modules:
    - Embedder: Converts prompts into dense vector embeddings.
    - Router: Classifies each conversation into a task type.
    - RAG: Retrieves documents and formats prompts for LLM input.
    - LLM: Generates responses from a language model.
    - ImageGenerator: Creates image outputs from prompts when needed.

    Args:
        router_model (torch.nn.Module): Pretrained classifier for task routing.
        rag_vector_db (VectorDatabaseBase): Vector DB used in retrieval-augmented generation.
        rag_n_extracted_docs (int): Number of docs to retrieve per query (default 5).
        rag_prompt_format (str): Format string to merge retrieved docs and prompt.
        embed_model_name (str): Model name for the embedder.
        embed_device_name (str): Device name for running the embedder (e.g., 'cuda').
        llm_model_name (str): Model name for the LLM backend.
        llm_dtype (str): Data type used by the LLM (e.g., 'int8').
        llm_max_new_tokens (int): Max number of tokens the LLM can generate.
        llm_temperature (float): Sampling temperature for the LLM.
        llm_device_name (str): Device used for running the LLM (e.g., 'cuda').
        llm_stream_output (bool): Whether to stream the LLM output incrementally.
        img_model_name (str): Model name for the image generation model.
        img_device_name (str): Device for image generation (e.g., 'cuda').
        img_dtype (str): Data type for image model (e.g., 'fp16').
        img_scheduler_type (str): Type of scheduler to use during image generation.
        img_use_refiner (bool): Whether to use an additional refiner pipeline.
        img_refiner_name (str): Model name of the image refiner pipeline.
    """
    _instance = None

    def __new__(cls, *args, **kwargs):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._initialized = False
        return cls._instance

    def __init__(self,
        router_model: torch.nn.Module,
        rag_vector_db: VectorDatabaseBase,
        rag_n_extracted_docs: int = 5,
        rag_prompt_format: str = '{context}\n{prompt}',
        embed_model_name: str = 'all-mpnet-base-v2',
        embed_device_name: str = 'cuda',
        llm_model_name: str = 'meta-llama/Llama-3.2-3B-Instruct',
        llm_dtype: str = 'int8',
        llm_max_new_tokens: int = 1024,
        llm_temperature: float = 0.7,
        llm_device_name: str = 'cuda',
        llm_stream_output: bool = False,
        img_model_name: str = '',
        img_device_name: str = 'cuda',
        img_dtype: str = 'fp16',
        img_scheduler_type: str = 'euler_ancestral',
        img_use_refiner: bool = False,
        img_refiner_name: str = 'stabilityai/stable-diffusion-xl-refiner-1.0'
    ):
        if hasattr(self, '_initialized') and self._initialized:
            return  # Avoid reinitialization
        
        # Instantiate core components
        self.embedder = Embedder(model_name=embed_model_name, device_name=embed_device_name)
        self.router = Router(model=router_model)
        self.rag = RAG(
            vector_db=rag_vector_db, 
            n_extracted_docs=rag_n_extracted_docs, 
            prompt_format=rag_prompt_format
        )
        self.llm = LLM(
            model_name=llm_model_name,
            dtype=llm_dtype,
            max_new_tokens=llm_max_new_tokens,
            temperature=llm_temperature,
            device_name=llm_device_name
        )
        self.image_generator = ImageGenerator(
            model_name=img_model_name,
            device=img_device_name,
            dtype=img_dtype,
            scheduler_type=img_scheduler_type,
            use_refiner=img_use_refiner,
            refiner_name=img_refiner_name
        )
        self.stream_output = llm_stream_output
        self._initialized = True

    def __call__(self, batch: ConversationBatch) -> List[AgentResponse]:
        """
        Processes a batch of conversations end-to-end and returns agent responses.

        Args:
            batch (ConversationBatch): Batch of conversation histories and metadata.

        Returns:
            List[AgentResponse]: List of responses generated by the LLM.
        """
        try:
            grouped_convs = self._route_and_group(batch)
            return self._process_task_batches(grouped_convs)
        except Exception:
            logger.error(f"Pipeline execution failed.", exc_info=True)
            return []

    def _route_and_group(self, batch: ConversationBatch) -> Dict[TaskType, TaskBatch]:
        """
        Embeds prompts, classifies them by task type, and groups them accordingly.
        """
        #  Convert message history into string prompts
        prompts = PromptTransformer.format_messages_to_str_batch(batch.histories)
        # Generate dense embeddings for the prompts
        embeddings = self.embedder.extract_embeddings(prompts)
        # Classify the embeddings into task types
        task_types, probs = self.router.route(embeddings, return_probs=True)

        logger.debug(f"Routed task types: {[t.name for t in task_types]}, Probabilities: {probs.tolist()}")

        # Group inputs into batches by predicted task type
        return group_conversations_by_task(
            messages=batch.histories,
            task_types=task_types,
            conv_ids=batch.conv_ids,
            embeddings=embeddings
        )

    def _process_task_batches(self, grouped_convs: Dict[TaskType, TaskBatch]) -> List[AgentResponse]:
        """Processes each group of conversations based on its task type."""
        responses: List[AgentResponse] = []

        for task_type, group in grouped_convs.items():
            logger.debug(f"Handling {task_type.name} with {len(group.histories)} conversations")

            if task_type == TaskType.RAG:
                responses.extend(self._handle_rag_task(group))
            if task_type == TaskType.IMAGE_GEN:
                responses.extend(self._handle_img_gen_task(group))
            else:
                logger.warning(f"Unsupported task type: {task_type.name}")

        return responses

    def _handle_rag_task(self, group: TaskBatch) -> List[AgentResponse]:
        """Executes RAG-specific logic: retrieval, prompt generation, and LLM response."""
        # Add retrieved documents to each conversation context
        self.rag.add_context(group.histories, group.embeddings)

        # Generate responses either as a stream or full text
        if self.stream_output:
            outputs = self.llm.generate_stream_batch(group.histories)
            response_type = "stream"
        else:
            outputs = self.llm.generate_batch(group.histories)
            response_type = "text"

        # Create AgentResponse objects for each output
        return [
            AgentResponse(conv_id=conv_id, type=response_type, content=content)
            for conv_id, content in zip(group.conv_ids, outputs)
        ]

    def _handle_img_gen_task(self, group: TaskBatch) -> List[AgentResponse]:
        """Executes image generation logic using prompts from user messages."""
        # Extract user prompts from the conversation history
        users_messages = PromptTransformer.get_messages_by_role(group.histories, role='user')
        prompts = PromptTransformer.get_content_from_messages(users_messages)

        # Generate images and convert them to base64
        imgs_tensor = self.image_generator.generate(prompts=prompts)
        outputs = [tensor_to_base64(img_tensor) for img_tensor in imgs_tensor]
        
        return [
            AgentResponse(conv_id=conv_id, type="image", content=content)
            for conv_id, content in zip(group.conv_ids, outputs)
        ]

    @classmethod
    def build(cls) -> 'Pipeline':
        """
        Builds and returns a pipeline instance using the configuration factory.

        Returns:
            Pipeline: Configured pipeline instance.
        """
        from orchestrator.pipeline.factory import PipelineFactory
        return PipelineFactory.build()