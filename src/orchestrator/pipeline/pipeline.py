from typing import Dict, List
import logging

import torch

from core.entities.types import AgentResponse, TaskBatch
from modules.llm import LLM
from modules.rag import RAG, VectorDatabaseBase
from orchestrator.pipeline.utils import group_conversations_by_task
from orchestrator.router import Router
from shared.embedders import Embedder
from shared.text_processing import PromptTransformer

from core.entities import ConversationBatch, TaskType

logger = logging.getLogger(__name__)


class Pipeline:
    """
    Singleton pipeline for processing batches of conversations through embedding, routing, and task execution.

    This class handles the high-level orchestration between core modules:
    - Embedder: Converts prompts into dense vector embeddings.
    - Router: Classifies each conversation into a task type.
    - RAG: Retrieves documents and formats prompts for LLM input.
    - LLM: Generates responses from a language model.

    Args:
        router_model (torch.nn.Module): Pretrained classifier for task routing.
        rag_vector_db (VectorDatabaseBase): Vector DB used in retrieval-augmented generation.
        rag_n_extracted_docs (int): Number of docs to retrieve per query (default 5).
        rag_prompt_format (str): Format string to merge retrieved docs and prompt.
        embed_model_name (str): Model name for the embedder.
        embed_device_name (str): Device name for running the embedder (e.g., 'cuda').
        llm_model_name (str): Model name for the LLM backend.
        llm_dtype (str): Data type used by the LLM (e.g., 'int8').
        llm_max_new_tokens (int): Max number of tokens the LLM can generate.
        llm_temperature (float): Sampling temperature for the LLM.
        llm_device_name (str): Device used for running the LLM (e.g., 'cuda').
        llm_stream_output (bool): Whether to stream the LLM output incrementally.
    """
    _instance = None

    def __new__(cls, *args, **kwargs):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._initialized = False
        return cls._instance

    def __init__(self,
        router_model: torch.nn.Module,
        rag_vector_db: VectorDatabaseBase,
        rag_n_extracted_docs: int = 5,
        rag_prompt_format: str = '{context}\n{prompt}',
        embed_model_name: str = 'all-mpnet-base-v2',
        embed_device_name: str = 'cuda',
        llm_model_name: str = 'meta-llama/Llama-3.2-3B-Instruct',
        llm_dtype: str = 'int8',
        llm_max_new_tokens: int = 1024,
        llm_temperature: float = 0.7,
        llm_device_name: str = 'cuda',
        llm_stream_output: bool = False
    ):
        if hasattr(self, '_initialized') and self._initialized:
            return
        
         # Instantiate core components
        self.embedder = Embedder(model_name=embed_model_name, device_name=embed_device_name)
        self.router = Router(model=router_model)
        self.rag = RAG(
            vector_db=rag_vector_db, 
            n_extracted_docs=rag_n_extracted_docs, 
            prompt_format=rag_prompt_format
        )
        self.llm = LLM(
            model_name=llm_model_name,
            dtype=llm_dtype,
            max_new_tokens=llm_max_new_tokens,
            temperature=llm_temperature,
            device_name=llm_device_name
        )
        self.stream_output = llm_stream_output
        self._initialized = True

    def __call__(self, batch: ConversationBatch) -> List[AgentResponse]:
        """
        Processes a batch of conversations end-to-end and returns agent responses.

        Args:
            batch (ConversationBatch): Batch of conversation histories and metadata.

        Returns:
            List[AgentResponse]: List of responses generated by the LLM.
        """
        try:
            grouped_convs = self._route_and_group(batch)
            return self._process_task_batches(grouped_convs)
        except Exception:
            logger.error(f"Pipeline execution failed.", exc_info=True)
            return []

    def _route_and_group(self, batch: ConversationBatch) -> Dict[TaskType, TaskBatch]:
        """
        Embeds prompts, classifies them by task type, and groups them accordingly.
        """
        #  Convert message history into string prompts
        prompts = PromptTransformer.format_messages_to_str_batch(batch.histories)
        # Generate dense embeddings for the prompts
        embeddings = self.embedder.extract_embeddings(prompts)
        # Classify the embeddings into task types
        task_types, probs = self.router.route(embeddings, return_probs=True)

        logger.debug(f"Routed task types: {[t.name for t in task_types]}, Probabilities: {probs.tolist()}")

        # Group inputs into batches by predicted task type
        return group_conversations_by_task(
            messages=batch.histories,
            task_types=task_types,
            conv_ids=batch.conv_ids,
            embeddings=embeddings
        )

    def _process_task_batches(self, grouped_convs: Dict[TaskType, TaskBatch]) -> List[AgentResponse]:
        """Processes each group of conversations based on its task type."""
        responses: List[AgentResponse] = []

        for task_type, group in grouped_convs.items():
            logger.debug(f"Handling {task_type.name} with {len(group.histories)} conversations")

            if task_type == TaskType.RAG:
                responses.extend(self._handle_rag_task(group))
            else:
                logger.warning(f"Unsupported task type: {task_type.name}")

        return responses

    def _handle_rag_task(self, group: TaskBatch) -> List[AgentResponse]:
        """Executes RAG-specific logic: retrieval, prompt generation, and LLM response."""
        # Add retrieved documents to each conversation context
        self.rag.add_context(group.histories, group.embeddings)

        # Generate responses either as a stream or full text
        if self.stream_output:
            outputs = self.llm.generate_stream_batch(group.histories)
            response_type = "stream"
        else:
            outputs = self.llm.generate_batch(group.histories)
            response_type = "text"

        # Create AgentResponse objects for each output
        return [
            AgentResponse(conv_id=conv_id, type=response_type, content=content)
            for conv_id, content in zip(group.conv_ids, outputs)
        ]

    @classmethod
    def build(cls) -> 'Pipeline':
        """
        Builds and returns a pipeline instance using the configuration factory.

        Returns:
            Pipeline: Configured pipeline instance.
        """
        from orchestrator.pipeline.factory import PipelineFactory
        return PipelineFactory.build()